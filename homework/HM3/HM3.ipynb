{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Chang Lu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "\n",
    "seed_value = 6666\n",
    "numpy.random.seed(seed_value)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    results = numpy.zeros((len(y), num_class))\n",
    "    for i, label in enumerate(y):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\tools\\python\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 32)        96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        192       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 64)        192       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 64)        192       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 128)         384       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 128)         384       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 128)         384       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,538,538\n",
      "Trainable params: 1,536,170\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, strides=1, padding='same', activation=None, input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(32, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(32, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-3 # to be tuned!\n",
    "\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\tools\\python\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 1.8971 - accuracy: 0.3747 - val_loss: 1.6116 - val_accuracy: 0.4359\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 1.3992 - accuracy: 0.5260 - val_loss: 1.9377 - val_accuracy: 0.5023\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 1.1690 - accuracy: 0.6034 - val_loss: 1.0023 - val_accuracy: 0.6477\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.9958 - accuracy: 0.6584 - val_loss: 0.9054 - val_accuracy: 0.6942\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.8843 - accuracy: 0.6920 - val_loss: 0.9621 - val_accuracy: 0.6715\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.8217 - accuracy: 0.7159 - val_loss: 0.7679 - val_accuracy: 0.7339\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.7573 - accuracy: 0.7393 - val_loss: 0.8043 - val_accuracy: 0.7420\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.7207 - accuracy: 0.7503 - val_loss: 0.6604 - val_accuracy: 0.7754\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6874 - accuracy: 0.7630 - val_loss: 0.9172 - val_accuracy: 0.6990\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6525 - accuracy: 0.7755 - val_loss: 1.0479 - val_accuracy: 0.6827\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6262 - accuracy: 0.7837 - val_loss: 0.6528 - val_accuracy: 0.7805\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5997 - accuracy: 0.7931 - val_loss: 0.6041 - val_accuracy: 0.7934\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5752 - accuracy: 0.8009 - val_loss: 0.7072 - val_accuracy: 0.7685\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5583 - accuracy: 0.8069 - val_loss: 0.5957 - val_accuracy: 0.8013\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5441 - accuracy: 0.8120 - val_loss: 0.5504 - val_accuracy: 0.8143\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5220 - accuracy: 0.8195 - val_loss: 0.5644 - val_accuracy: 0.8084\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5095 - accuracy: 0.8207 - val_loss: 0.7079 - val_accuracy: 0.7874\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4959 - accuracy: 0.8299 - val_loss: 0.5169 - val_accuracy: 0.8286\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4739 - accuracy: 0.8364 - val_loss: 0.5737 - val_accuracy: 0.8064\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4676 - accuracy: 0.8376 - val_loss: 0.4941 - val_accuracy: 0.8380\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4510 - accuracy: 0.8444 - val_loss: 0.5038 - val_accuracy: 0.8323\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4441 - accuracy: 0.8468 - val_loss: 0.7555 - val_accuracy: 0.7777\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4357 - accuracy: 0.8491 - val_loss: 0.4742 - val_accuracy: 0.8454\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4245 - accuracy: 0.8533 - val_loss: 0.5120 - val_accuracy: 0.8296\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4163 - accuracy: 0.8559 - val_loss: 0.5474 - val_accuracy: 0.8242\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4047 - accuracy: 0.8613 - val_loss: 0.4207 - val_accuracy: 0.8571\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3982 - accuracy: 0.8610 - val_loss: 0.5229 - val_accuracy: 0.8345\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3856 - accuracy: 0.8666 - val_loss: 0.4228 - val_accuracy: 0.86191s - loss: 0.3841 - accuracy -\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3838 - accuracy: 0.8663 - val_loss: 0.4412 - val_accuracy: 0.8620\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3722 - accuracy: 0.8718 - val_loss: 0.5325 - val_accuracy: 0.8360705 - accuracy:  - ETA: 0s - loss: 0.3709 - \n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3694 - accuracy: 0.8708 - val_loss: 0.4378 - val_accuracy: 0.8595\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3674 - accuracy: 0.8723 - val_loss: 0.4743 - val_accuracy: 0.8513\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3507 - accuracy: 0.8803 - val_loss: 0.4693 - val_accuracy: 0.85527 - accuracy: \n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3502 - accuracy: 0.8794 - val_loss: 0.4207 - val_accuracy: 0.8664\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3375 - accuracy: 0.8825 - val_loss: 0.4274 - val_accuracy: 0.8621\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3337 - accuracy: 0.8848 - val_loss: 0.3837 - val_accuracy: 0.8747\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3287 - accuracy: 0.8849 - val_loss: 0.4415 - val_accuracy: 0.8607\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3239 - accuracy: 0.8884 - val_loss: 0.3621 - val_accuracy: 0.8819\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3205 - accuracy: 0.8884 - val_loss: 0.3761 - val_accuracy: 0.8760\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3143 - accuracy: 0.8909 - val_loss: 0.4369 - val_accuracy: 0.8632\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3127 - accuracy: 0.8912 - val_loss: 0.4155 - val_accuracy: 0.8663\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.3090 - accuracy: 0.8936 - val_loss: 0.4182 - val_accuracy: 0.8715\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2948 - accuracy: 0.8968 - val_loss: 0.3997 - val_accuracy: 0.8747 - loss: 0.2937 - ac\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2958 - accuracy: 0.8968 - val_loss: 0.3697 - val_accuracy: 0.8791accu - ETA -\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2927 - accuracy: 0.8990 - val_loss: 0.3783 - val_accuracy: 0.8797\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2907 - accuracy: 0.8983 - val_loss: 0.4716 - val_accuracy: 0.8593\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2878 - accuracy: 0.9002 - val_loss: 0.4173 - val_accuracy: 0.8701\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 0.2825 - accuracy: 0.9007 - val_loss: 0.4362 - val_accuracy: 0.8642\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2721 - accuracy: 0.9068 - val_loss: 0.3835 - val_accuracy: 0.8768\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2720 - accuracy: 0.9057 - val_loss: 0.4162 - val_accuracy: 0.8724\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2669 - accuracy: 0.9072 - val_loss: 0.4326 - val_accuracy: 0.8698\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2650 - accuracy: 0.9066 - val_loss: 0.4158 - val_accuracy: 0.8738: 0s - loss: 0.2647 - accuracy: 0.\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2623 - accuracy: 0.9075 - val_loss: 0.3863 - val_accuracy: 0.8816\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2560 - accuracy: 0.9109 - val_loss: 0.4123 - val_accuracy: 0.8705\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2558 - accuracy: 0.9093 - val_loss: 0.4064 - val_accuracy: 0.8814\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2527 - accuracy: 0.9118 - val_loss: 0.4247 - val_accuracy: 0.8762\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2504 - accuracy: 0.9128 - val_loss: 0.4343 - val_accuracy: 0.8689\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2481 - accuracy: 0.9125 - val_loss: 0.4276 - val_accuracy: 0.8742\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2427 - accuracy: 0.9153 - val_loss: 0.3687 - val_accuracy: 0.8895\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2434 - accuracy: 0.9154 - val_loss: 0.4434 - val_accuracy: 0.8668\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2347 - accuracy: 0.9179 - val_loss: 0.4382 - val_accuracy: 0.8729\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2407 - accuracy: 0.9151 - val_loss: 0.4015 - val_accuracy: 0.8813\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2315 - accuracy: 0.9177 - val_loss: 0.3957 - val_accuracy: 0.8806\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2332 - accuracy: 0.9173 - val_loss: 0.4168 - val_accuracy: 0.8772\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2293 - accuracy: 0.9196 - val_loss: 0.3415 - val_accuracy: 0.8927\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2260 - accuracy: 0.9204 - val_loss: 0.4323 - val_accuracy: 0.8713\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2234 - accuracy: 0.9212 - val_loss: 0.3564 - val_accuracy: 0.8921\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2249 - accuracy: 0.9216 - val_loss: 0.3753 - val_accuracy: 0.8852\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2230 - accuracy: 0.9223 - val_loss: 0.3674 - val_accuracy: 0.8882 loss: 0.2221 - accuracy\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2199 - accuracy: 0.9234 - val_loss: 0.3886 - val_accuracy: 0.8853\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2154 - accuracy: 0.9241 - val_loss: 0.3662 - val_accuracy: 0.8878\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2137 - accuracy: 0.9240 - val_loss: 0.3743 - val_accuracy: 0.8873\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2115 - accuracy: 0.9254 - val_loss: 0.4303 - val_accuracy: 0.8780\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2115 - accuracy: 0.9259 - val_loss: 0.3814 - val_accuracy: 0.8863\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2083 - accuracy: 0.9279 - val_loss: 0.3832 - val_accuracy: 0.8848\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2094 - accuracy: 0.9262 - val_loss: 0.4049 - val_accuracy: 0.8837 - loss: 0.2100 - accu - ETA: 1s - l - ETA: 1s - loss: 0.2 - ETA: 0s - loss: 0.2091 - accu\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2129 - accuracy: 0.9253 - val_loss: 0.4395 - val_accuracy: 0.8774\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.2035 - accuracy: 0.9283 - val_loss: 0.3800 - val_accuracy: 0.8872oss: 0.2034 - accuracy: 0.\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1971 - accuracy: 0.9321 - val_loss: 0.3902 - val_accuracy: 0.8912\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1974 - accuracy: 0.9304 - val_loss: 0.3653 - val_accuracy: 0.8931\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1997 - accuracy: 0.9311 - val_loss: 0.3902 - val_accuracy: 0.8879\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1993 - accuracy: 0.9299 - val_loss: 0.4165 - val_accuracy: 0.8828\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1922 - accuracy: 0.9309 - val_loss: 0.4489 - val_accuracy: 0.8716\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1889 - accuracy: 0.9341 - val_loss: 0.3838 - val_accuracy: 0.8881TA: 0s - loss: 0.189\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1904 - accuracy: 0.9330 - val_loss: 0.3294 - val_accuracy: 0.8995\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1928 - accuracy: 0.9335 - val_loss: 0.3880 - val_accuracy: 0.8889\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1928 - accuracy: 0.9323 - val_loss: 0.3837 - val_accuracy: 0.8921curacy: \n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1846 - accuracy: 0.9351 - val_loss: 0.3788 - val_accuracy: 0.8909\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1805 - accuracy: 0.9363 - val_loss: 0.3894 - val_accuracy: 0.8877\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1880 - accuracy: 0.9344 - val_loss: 0.3592 - val_accuracy: 0.8933 -\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1859 - accuracy: 0.9354 - val_loss: 0.4014 - val_accuracy: 0.8872\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1829 - accuracy: 0.9360 - val_loss: 0.3736 - val_accuracy: 0.8939 - loss: 0.1808 - accuracy - ETA: 1s - loss: 0.1810 - accuracy: 0.93 - ETA: 1s - loss: 0.1814 - accuracy: 0.93 - ETA\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1801 - accuracy: 0.9360 - val_loss: 0.3703 - val_accuracy: 0.89120. - ETA\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1824 - accuracy: 0.9360 - val_loss: 0.3656 - val_accuracy: 0.8939\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1789 - accuracy: 0.9359 - val_loss: 0.4129 - val_accuracy: 0.8898\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1785 - accuracy: 0.9368 - val_loss: 0.3896 - val_accuracy: 0.8923 loss: 0.1779 \n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1740 - accuracy: 0.9394 - val_loss: 0.3953 - val_accuracy: 0.8910\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1745 - accuracy: 0.9376 - val_loss: 0.3727 - val_accuracy: 0.8944os\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1724 - accuracy: 0.9406 - val_loss: 0.3459 - val_accuracy: 0.8978\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1739 - accuracy: 0.9390 - val_loss: 0.3485 - val_accuracy: 0.8974\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        zca_epsilon=1e-06,\n",
    "        rotation_range=0,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,\n",
    "        zoom_range=0.,\n",
    "        channel_shift_range=0.,\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        rescale=None,\n",
    "        preprocessing_function=None,\n",
    "        data_format=None,\n",
    "        validation_split=0.0)\n",
    "\n",
    "datagen.fit(x_tr)\n",
    "history = model.fit_generator(datagen.flow(x_tr, y_tr, batch_size=128),\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=100)\n",
    "\n",
    "# history = model.fit(x_tr, y_tr, batch_size=128, epochs=100, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, strides=1, padding='same', activation=None, input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(32, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(32, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(128, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(scale=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 1.8477 - accuracy: 0.38892s - los\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.2964 - accuracy: 0.5575 - E - - ETA: 2s - loss: 1\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.0476 - accuracy: 0.6425\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9132 - accuracy: 0.6825\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.8162 - accuracy: 0.7149\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.7515 - accuracy: 0.7377\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.7001 - accuracy: 0.75634s - loss: 0.7046 - ac - ETA: 3s - loss: 0.7033 - accuracy - ETA: 2s - loss: 0\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6571 - accuracy: 0.7708\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6292 - accuracy: 0.78233s - l - ETA:  - ETA: 0s - loss: 0.6295 - accuracy: 0.78\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6015 - accuracy: 0.7905\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5759 - accuracy: 0.7994\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5522 - accuracy: 0.8090\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.5354 - accuracy: 0.81377s - loss: 0.529 - ETA: 6s - loss: - ETA: 3s - loss: 0.5351 - accura\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5175 - accuracy: 0.8200\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5003 - accuracy: 0.82801s - loss: 0.5007 - accuracy - ETA: 1s - loss: 0.4996 - accu - ETA: 0s - loss: 0.500\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4811 - accuracy: 0.8342\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.4703 - accuracy: 0.8373\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4571 - accuracy: 0.8416\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4446 - accuracy: 0.84671s - loss: 0 - ETA: 0s - loss: 0.444 - ETA: 0s - loss: 0.4450 - accu\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4391 - accuracy: 0.8468\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4274 - accuracy: 0.85220s - loss: 0.4271 - accura\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4137 - accuracy: 0.8561 - ETA: 3s - loss: 0 - ETA: 1s - loss: 0.4129  - ETA: 1s - loss: - ETA: 0s - loss: 0.4139 - accuracy\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4013 - accuracy: 0.8623\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3932 - accuracy: 0.8636\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3887 - accuracy: 0.8653\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3764 - accuracy: 0.86992s - l - E - ETA: 0s - loss: 0.3780 - accura - ETA: 0s - loss: 0.3769 - accuracy: \n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3697 - accuracy: 0.87150s - loss: 0.3696 - accuracy: 0.87\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3623 - accuracy: 0.87522s - loss: 0.358 - ETA: 1s - los - ETA: 1s\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3578 - accuracy: 0.8753TA: 3s - loss: 0.3538 - accuracy: 0.87 - ETA: 3s - l - ETA: 2s - l - ETA: 1s - loss: 0.3551 - accura\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3551 - accuracy: 0.87780s - loss: 0.3539 - accuracy:  - ETA: 0s - loss: 0.3530 - \n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3426 - accuracy: 0.8827: 1s - loss: - ETA: 0s - loss: 0.3410 - ac\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3376 - accuracy: 0.8818\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3343 - accuracy: 0.8847\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3297 - accuracy: 0.88580s - loss: 0.3302 - accuracy\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3201 - accuracy: 0.88970s - loss: 0.3\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3160 - accuracy: 0.8915\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3111 - accuracy: 0.89330s - l\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3070 - accuracy: 0.8949\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.3031 - accuracy: 0.89520s - loss: 0.3029 - accura\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2949 - accuracy: 0.8952\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.2973 - accuracy: 0.89750s - loss: 0.2963 - ac\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2915 - accuracy: 0.89910s - l\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.2857 - accuracy: 0.9005\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2798 - accuracy: 0.9018\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.2765 - accuracy: 0.90284s - l - ETA: 3s - l - ETA: 3s - loss: 0.2772 - ac - ETA: 1s - loss: 0 - ETA: 0s - loss: 0.2770 - ac\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2766 - accuracy: 0.90330s - loss: 0.2768 - ac\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2639 - accuracy: 0.9083\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2634 - accuracy: 0.90991s - l - ETA: 0s - loss: 0.2642 - accu\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2646 - accuracy: 0.9080\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2610 - accuracy: 0.90910s - l\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2574 - accuracy: 0.90993s - loss: 0.2546 - ac - ETA: 3s - loss: 0.2547 - ac - ETA: 2s - loss: 0.2552 - accuracy: 0. - ETA: 2s - loss: 0.2 - ETA: 2s - loss: 0.2557 - accuracy: 0.90 - ETA: 2s - loss: 0.2555 - accuracy:  - ETA: 0s - loss: 0.2561 \n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2574 - accuracy: 0.91168s - loss: 0.2536 - ac - ETA: 4s - loss: 0.2547 -  - ETA: 4s - los - ETA: 2s - loss: 0.2 - ETA: 1s - los - ETA: 0s - loss: 0\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2494 - accuracy: 0.9146: 10s - ETA: 0s - loss:\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2468 - accuracy: 0.91413s - loss: 0.2424 - accuracy - ETA: 0s - loss: 0.2464 - accuracy: 0.91 - ETA: 0s - loss: 0.2468 - ac\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2450 - accuracy: 0.91410s - loss: 0 - ETA: 0s - loss: 0.2448 - accuracy\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2408 - accuracy: 0.91490s - loss: 0.2406 - accuracy: \n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2413 - accuracy: 0.9159: 14s - loss: 0.2522 - accuracy: 0.917 - ETA: 13s - - ETA: 10s - loss: 0.2283 - accuracy: 0 - ETA: 9s - loss: 0.2306 - accuracy: 0. - ETA: 9s - loss: 0.2310 - accuracy:  - ETA - ETA: 6s - loss: 0.2358 - accuracy - ETA\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2424 - accuracy: 0.9146\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2342 - accuracy: 0.91610s - los\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2272 - accuracy: 0.9204 ETA: 1s - loss: 0 - ETA: 0s - loss: 0.2279 - accu - ETA: 0s - loss: 0.2276 \n",
      "Epoch 61/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.2334 - accuracy: 0.9190: 12s - los - ETA: 7s - loss: - ETA:  - ETA: \n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2282 - accuracy: 0.9202\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2255 - accuracy: 0.92021s - loss: 0.2241 - ac - ETA: 1s - loss: 0.2 - ETA: 0s - loss: 0.2\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2258 - accuracy: 0.92082s - loss: 0.2256 - accu - ETA: 2s - loss: 0.2247 - accura - ETA: 2s - loss: 0\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2215 - accuracy: 0.92324s - - - ETA: \n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2227 - accuracy: 0.9220: \n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2198 - accuracy: 0.92323s - loss: 0.2201 \n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2143 - accuracy: 0.9250\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2146 - accuracy: 0.92530s - los\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2106 - accuracy: 0.9256\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2135 - accuracy: 0.92462s - loss: 0.2105  - ETA: 1s - loss: 0.2118  - ETA: \n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2117 - accuracy: 0.92540s - loss:\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2061 - accuracy: 0.92853s - loss: 0 - ETA - ETA: 0s - loss: 0.2066 - accu\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2071 - accuracy: 0.9268\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2038 - accuracy: 0.9281\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1964 - accuracy: 0.93120s - loss: 0.1963 - accuracy: 0.\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2031 - accuracy: 0.9290\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1956 - accuracy: 0.93141s - loss: 0.1934 - accuracy - ETA: \n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1981 - accuracy: 0.9306\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1924 - accuracy: 0.9334\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1910 - accuracy: 0.9334: 6s - l - ETA: 5s - loss: - ETA: 4s - loss:\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1896 - accuracy: 0.9326\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1970 - accuracy: 0.9307\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1884 - accuracy: 0.9343\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1881 - accuracy: 0.9341\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1837 - accuracy: 0.93510s - loss: 0.1832 - accuracy\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1880 - accuracy: 0.93410s - loss: 0.1882 - accuracy: 0.93\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1840 - accuracy: 0.9364\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1837 - accuracy: 0.9356\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1799 - accuracy: 0.9356\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1824 - accuracy: 0.9365\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1794 - accuracy: 0.9369\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1760 - accuracy: 0.93870s - loss: 0.1751  - ETA: 0s - loss: 0.1763 - accuracy\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1773 - accuracy: 0.9373\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.1758 - accuracy: 0.93790s - loss: 0.1753 - ac\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1739 - accuracy: 0.93874s - - ETA: 3s - loss: 0.1761 -  - ETA: 2s - loss: 0.1759 - accuracy: 0. - ETA: 2s - los - ETA: 0s - loss: 0.174\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1707 - accuracy: 0.9389\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1662 - accuracy: 0.9425\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1707 - accuracy: 0.9402\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1682 - accuracy: 0.9416\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        zca_epsilon=1e-06,\n",
    "        rotation_range=0,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,\n",
    "        zoom_range=0.,\n",
    "        channel_shift_range=0.,\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        rescale=None,\n",
    "        preprocessing_function=None,\n",
    "        data_format=None,\n",
    "        validation_split=0.0)\n",
    "\n",
    "datagen.fit(x_tr)\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train_vec, batch_size=128),\n",
    "                        epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 116us/step\n",
      "loss = 0.4305327400714159\n",
      "accuracy = 0.887499988079071\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
